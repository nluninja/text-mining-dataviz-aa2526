{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "1. NLP in Practice\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis on Text Data\n",
    "\n",
    "In this notebook we'll apply everything we've learned so far to perform **Exploratory Data Analysis (EDA)** on a real text dataset. We'll use spaCy for NLP processing and matplotlib/seaborn for visualization.\n",
    "\n",
    "**Dataset:** 20 Newsgroups - a classic dataset for text classification containing ~20,000 newsgroup posts across 20 topics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U spacy\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "\n",
    "# Set style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "\n",
    "# Load spaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the Dataset\n",
    "\n",
    "The 20 Newsgroups dataset is available directly from scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "# Load a subset of categories for faster processing\n",
    "categories = ['sci.space', 'rec.sport.baseball', 'comp.graphics', 'talk.politics.misc']\n",
    "\n",
    "newsgroups = fetch_20newsgroups(\n",
    "    subset='train',\n",
    "    categories=categories,\n",
    "    remove=('headers', 'footers', 'quotes')  # Remove metadata for cleaner text\n",
    ")\n",
    "\n",
    "print(f\"Number of documents: {len(newsgroups.data)}\")\n",
    "print(f\"Categories: {newsgroups.target_names}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'text': newsgroups.data,\n",
    "    'category': [newsgroups.target_names[i] for i in newsgroups.target]\n",
    "})\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at a sample document\n",
    "print(\"Sample document:\")\n",
    "print(\"-\" * 50)\n",
    "print(df['text'].iloc[0][:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add text length columns\n",
    "df['char_count'] = df['text'].apply(len)\n",
    "df['word_count'] = df['text'].apply(lambda x: len(x.split()))\n",
    "\n",
    "df[['category', 'char_count', 'word_count']].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics\n",
    "df[['char_count', 'word_count']].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Category Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count documents per category\n",
    "category_counts = df['category'].value_counts()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=category_counts.values, y=category_counts.index, palette='viridis')\n",
    "plt.xlabel('Number of Documents')\n",
    "plt.ylabel('Category')\n",
    "plt.title('Document Distribution by Category')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Length Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Character count distribution\n",
    "axes[0].hist(df['char_count'], bins=50, edgecolor='black', alpha=0.7)\n",
    "axes[0].set_xlabel('Character Count')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "axes[0].set_title('Distribution of Character Count')\n",
    "axes[0].axvline(df['char_count'].median(), color='red', linestyle='--', label=f'Median: {df[\"char_count\"].median():.0f}')\n",
    "axes[0].legend()\n",
    "\n",
    "# Word count distribution\n",
    "axes[1].hist(df['word_count'], bins=50, edgecolor='black', alpha=0.7, color='orange')\n",
    "axes[1].set_xlabel('Word Count')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "axes[1].set_title('Distribution of Word Count')\n",
    "axes[1].axvline(df['word_count'].median(), color='red', linestyle='--', label=f'Median: {df[\"word_count\"].median():.0f}')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text length by category\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.boxplot(data=df, x='category', y='word_count', palette='Set2')\n",
    "plt.xlabel('Category')\n",
    "plt.ylabel('Word Count')\n",
    "plt.title('Word Count Distribution by Category')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Processing with spaCy\n",
    "\n",
    "Now let's process the texts with spaCy to extract linguistic features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process a sample of documents (full dataset would take too long)\n",
    "# We'll use nlp.pipe for efficiency\n",
    "\n",
    "sample_size = 500\n",
    "df_sample = df.sample(n=sample_size, random_state=42).reset_index(drop=True)\n",
    "\n",
    "print(f\"Processing {sample_size} documents with spaCy...\")\n",
    "\n",
    "# Process with spaCy\n",
    "docs = list(nlp.pipe(df_sample['text'], batch_size=50))\n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Token Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract tokens (excluding punctuation and spaces)\n",
    "all_tokens = []\n",
    "all_tokens_no_stop = []\n",
    "\n",
    "for doc in docs:\n",
    "    for token in doc:\n",
    "        if not token.is_punct and not token.is_space:\n",
    "            all_tokens.append(token.text.lower())\n",
    "            if not token.is_stop:\n",
    "                all_tokens_no_stop.append(token.text.lower())\n",
    "\n",
    "print(f\"Total tokens: {len(all_tokens)}\")\n",
    "print(f\"Tokens without stopwords: {len(all_tokens_no_stop)}\")\n",
    "print(f\"Unique tokens: {len(set(all_tokens))}\")\n",
    "print(f\"Unique tokens (no stopwords): {len(set(all_tokens_no_stop))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 20 most common tokens (with stopwords)\n",
    "token_freq = Counter(all_tokens)\n",
    "top_tokens = token_freq.most_common(20)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "words, counts = zip(*top_tokens)\n",
    "sns.barplot(x=list(counts), y=list(words), palette='Blues_d')\n",
    "plt.xlabel('Frequency')\n",
    "plt.ylabel('Token')\n",
    "plt.title('Top 20 Most Common Tokens (with stopwords)')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 20 most common tokens (without stopwords)\n",
    "token_freq_no_stop = Counter(all_tokens_no_stop)\n",
    "top_tokens_no_stop = token_freq_no_stop.most_common(20)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "words, counts = zip(*top_tokens_no_stop)\n",
    "sns.barplot(x=list(counts), y=list(words), palette='Greens_d')\n",
    "plt.xlabel('Frequency')\n",
    "plt.ylabel('Token')\n",
    "plt.title('Top 20 Most Common Tokens (without stopwords)')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemma Analysis\n",
    "\n",
    "Comparing tokens vs lemmas to see the effect of lemmatization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract lemmas\n",
    "all_lemmas = []\n",
    "\n",
    "for doc in docs:\n",
    "    for token in doc:\n",
    "        if not token.is_punct and not token.is_space and not token.is_stop:\n",
    "            all_lemmas.append(token.lemma_.lower())\n",
    "\n",
    "print(f\"Unique tokens (no stopwords): {len(set(all_tokens_no_stop))}\")\n",
    "print(f\"Unique lemmas (no stopwords): {len(set(all_lemmas))}\")\n",
    "print(f\"Vocabulary reduction: {(1 - len(set(all_lemmas))/len(set(all_tokens_no_stop)))*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 20 most common lemmas\n",
    "lemma_freq = Counter(all_lemmas)\n",
    "top_lemmas = lemma_freq.most_common(20)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "words, counts = zip(*top_lemmas)\n",
    "sns.barplot(x=list(counts), y=list(words), palette='Oranges_d')\n",
    "plt.xlabel('Frequency')\n",
    "plt.ylabel('Lemma')\n",
    "plt.title('Top 20 Most Common Lemmas (without stopwords)')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part of Speech (POS) Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract POS tags\n",
    "pos_tags = []\n",
    "\n",
    "for doc in docs:\n",
    "    for token in doc:\n",
    "        if not token.is_punct and not token.is_space:\n",
    "            pos_tags.append(token.pos_)\n",
    "\n",
    "pos_freq = Counter(pos_tags)\n",
    "print(\"POS Tag Distribution:\")\n",
    "for pos, count in pos_freq.most_common():\n",
    "    print(f\"  {pos}: {count} ({count/len(pos_tags)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# POS distribution visualization\n",
    "pos_df = pd.DataFrame(pos_freq.most_common(), columns=['POS', 'Count'])\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(data=pos_df, x='POS', y='Count', palette='Spectral')\n",
    "plt.xlabel('Part of Speech')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Part of Speech Distribution')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# POS distribution as pie chart\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.pie(pos_df['Count'], labels=pos_df['POS'], autopct='%1.1f%%', startangle=90)\n",
    "plt.title('Part of Speech Distribution')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Named Entity Recognition (NER) Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract named entities\n",
    "entities = []\n",
    "entity_labels = []\n",
    "\n",
    "for doc in docs:\n",
    "    for ent in doc.ents:\n",
    "        entities.append(ent.text)\n",
    "        entity_labels.append(ent.label_)\n",
    "\n",
    "print(f\"Total entities found: {len(entities)}\")\n",
    "print(f\"Unique entities: {len(set(entities))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entity type distribution\n",
    "label_freq = Counter(entity_labels)\n",
    "label_df = pd.DataFrame(label_freq.most_common(), columns=['Entity Type', 'Count'])\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(data=label_df, x='Entity Type', y='Count', palette='coolwarm')\n",
    "plt.xlabel('Entity Type')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Named Entity Type Distribution')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top entities by type\n",
    "entity_df = pd.DataFrame({'entity': entities, 'label': entity_labels})\n",
    "\n",
    "# Top PERSON entities\n",
    "print(\"Top 10 PERSON entities:\")\n",
    "print(entity_df[entity_df['label'] == 'PERSON']['entity'].value_counts().head(10))\n",
    "print()\n",
    "\n",
    "# Top ORG entities\n",
    "print(\"Top 10 ORG entities:\")\n",
    "print(entity_df[entity_df['label'] == 'ORG']['entity'].value_counts().head(10))\n",
    "print()\n",
    "\n",
    "# Top GPE (geopolitical entity) entities\n",
    "print(\"Top 10 GPE entities:\")\n",
    "print(entity_df[entity_df['label'] == 'GPE']['entity'].value_counts().head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize top entities\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "for ax, label, color in zip(axes, ['PERSON', 'ORG', 'GPE'], ['Blues_d', 'Greens_d', 'Reds_d']):\n",
    "    top_ents = entity_df[entity_df['label'] == label]['entity'].value_counts().head(10)\n",
    "    if len(top_ents) > 0:\n",
    "        sns.barplot(x=top_ents.values, y=top_ents.index, palette=color, ax=ax)\n",
    "        ax.set_xlabel('Frequency')\n",
    "        ax.set_ylabel('Entity')\n",
    "        ax.set_title(f'Top 10 {label} Entities')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Cloud Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "\n",
    "# Create word cloud from lemmas (no stopwords)\n",
    "text_for_wordcloud = ' '.join(all_lemmas)\n",
    "\n",
    "wordcloud = WordCloud(\n",
    "    width=1200,\n",
    "    height=600,\n",
    "    background_color='white',\n",
    "    colormap='viridis',\n",
    "    max_words=100\n",
    ").generate(text_for_wordcloud)\n",
    "\n",
    "plt.figure(figsize=(15, 8))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.title('Word Cloud (Lemmas, no stopwords)', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word cloud per category\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "for ax, category in zip(axes.flat, df_sample['category'].unique()):\n",
    "    # Get documents for this category\n",
    "    category_indices = df_sample[df_sample['category'] == category].index.tolist()\n",
    "    category_docs = [docs[i] for i in category_indices if i < len(docs)]\n",
    "    \n",
    "    # Extract lemmas\n",
    "    category_lemmas = []\n",
    "    for doc in category_docs:\n",
    "        for token in doc:\n",
    "            if not token.is_punct and not token.is_space and not token.is_stop:\n",
    "                category_lemmas.append(token.lemma_.lower())\n",
    "    \n",
    "    if category_lemmas:\n",
    "        text = ' '.join(category_lemmas)\n",
    "        wc = WordCloud(width=600, height=400, background_color='white', colormap='viridis').generate(text)\n",
    "        ax.imshow(wc, interpolation='bilinear')\n",
    "        ax.set_title(category, fontsize=14)\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.suptitle('Word Clouds by Category', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentence Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze sentence lengths\n",
    "sentence_lengths = []\n",
    "sentences_per_doc = []\n",
    "\n",
    "for doc in docs:\n",
    "    doc_sentences = list(doc.sents)\n",
    "    sentences_per_doc.append(len(doc_sentences))\n",
    "    for sent in doc_sentences:\n",
    "        sentence_lengths.append(len(sent))\n",
    "\n",
    "print(f\"Total sentences: {len(sentence_lengths)}\")\n",
    "print(f\"Average sentence length: {np.mean(sentence_lengths):.1f} tokens\")\n",
    "print(f\"Average sentences per document: {np.mean(sentences_per_doc):.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Sentence length distribution\n",
    "axes[0].hist(sentence_lengths, bins=50, edgecolor='black', alpha=0.7, color='purple')\n",
    "axes[0].set_xlabel('Sentence Length (tokens)')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "axes[0].set_title('Distribution of Sentence Length')\n",
    "axes[0].axvline(np.mean(sentence_lengths), color='red', linestyle='--', label=f'Mean: {np.mean(sentence_lengths):.1f}')\n",
    "axes[0].legend()\n",
    "\n",
    "# Sentences per document\n",
    "axes[1].hist(sentences_per_doc, bins=30, edgecolor='black', alpha=0.7, color='teal')\n",
    "axes[1].set_xlabel('Sentences per Document')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "axes[1].set_title('Distribution of Sentences per Document')\n",
    "axes[1].axvline(np.mean(sentences_per_doc), color='red', linestyle='--', label=f'Mean: {np.mean(sentences_per_doc):.1f}')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dependency Parsing Visualization\n",
    "\n",
    "Let's visualize the dependency structure of a sample sentence using displacy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy import displacy\n",
    "\n",
    "# Find a good sample sentence (not too long, not too short)\n",
    "sample_doc = docs[0]\n",
    "sample_sentences = [sent for sent in sample_doc.sents if 5 < len(sent) < 15]\n",
    "\n",
    "if sample_sentences:\n",
    "    sample_sent = sample_sentences[0]\n",
    "    print(f\"Sample sentence: {sample_sent}\")\n",
    "    print()\n",
    "    displacy.render(sample_sent, style='dep', jupyter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NER Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find a document with entities\n",
    "for doc in docs[:20]:\n",
    "    if len(doc.ents) >= 3:\n",
    "        print(\"Sample document with entities:\")\n",
    "        displacy.render(doc[:200], style='ent', jupyter=True)\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a summary table\n",
    "summary = {\n",
    "    'Total Documents': len(df),\n",
    "    'Sample Size (processed)': sample_size,\n",
    "    'Categories': len(df['category'].unique()),\n",
    "    'Avg Words per Document': f\"{df['word_count'].mean():.1f}\",\n",
    "    'Total Tokens (sample)': len(all_tokens),\n",
    "    'Unique Tokens (sample)': len(set(all_tokens)),\n",
    "    'Unique Lemmas (sample)': len(set(all_lemmas)),\n",
    "    'Vocabulary Reduction (lemmatization)': f\"{(1 - len(set(all_lemmas))/len(set(all_tokens_no_stop)))*100:.1f}%\",\n",
    "    'Total Entities Found': len(entities),\n",
    "    'Unique Entity Types': len(set(entity_labels)),\n",
    "    'Avg Sentences per Document': f\"{np.mean(sentences_per_doc):.1f}\",\n",
    "    'Avg Tokens per Sentence': f\"{np.mean(sentence_lengths):.1f}\"\n",
    "}\n",
    "\n",
    "summary_df = pd.DataFrame(list(summary.items()), columns=['Metric', 'Value'])\n",
    "summary_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Key Takeaways\n",
    "\n",
    "In this EDA we:\n",
    "\n",
    "1. **Loaded and explored** the 20 Newsgroups dataset\n",
    "2. **Analyzed text length** distributions (characters, words, sentences)\n",
    "3. **Compared preprocessing** effects (tokens vs lemmas, with/without stopwords)\n",
    "4. **Examined POS tag** distributions\n",
    "5. **Extracted and visualized** named entities\n",
    "6. **Created word clouds** for the overall corpus and by category\n",
    "7. **Used displacy** to visualize dependency parsing and NER\n",
    "\n",
    "These techniques form the foundation for understanding any text dataset before building models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
